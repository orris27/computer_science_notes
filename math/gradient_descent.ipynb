{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does $x^{t+1} \\gets x^t - \\eta \\frac{\\partial f(x^t)}{\\partial x^t}$ minimizes $f(x)$?\n",
    "\n",
    "Taylor's Series:\n",
    "$$\n",
    "f(x) = f(x_0) + \\frac{f^{(1)}(x_0)}{1!} (x - x_0) + \\frac{f^{(2)}(x_0)}{2!} (x - x_0)^2 + \\cdots\n",
    "$$\n",
    "\n",
    "Multivariable Taylor's Series:\n",
    "$$\n",
    "f(x, y) = f(x_0, y_0) + \\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial x} (x - x_0) + \\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial y} (y - y_0) + \\cdots\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the constant $f(x_0, y_0)$, $\\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial x}$ and $\\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial y}$ with $s$, $u$ and $v$ respectively. Therefore, multivariable Taylor's series become:\n",
    "$$\n",
    "f(x, y) = s + u (x - x_0) + v (y - y_0) + \\cdots\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the linear approximation is constrained by:\n",
    "$$\n",
    "(x - x_0)^2 + (y - y_0)^2 \\leq d,\n",
    "$$\n",
    "where $d \\gt 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We denote $(x - x_0)$ and $(y - y_0)$ as $\\Delta x$ and $\\Delta y$, and thus we are minimizing the following formula:\n",
    "\\begin{aligned}\n",
    "& \\text{minimize} & &  u \\Delta x + v \\Delta y  \\\\\n",
    "& \\text{subjective to } & &  {\\Delta x}^2 +  {\\Delta y}^2 \\leq d, d \\gt 0\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is obvious that $(\\Delta x, \\Delta y)$ should be selected as $(-u, -v)$ from the figure below:\n",
    "![details](https://github.com/orris27/computer_science_notes/raw/master/images/gradient_descent_theory_final.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\Delta x$ and $\\Delta y$ represent the increment of $x$ and $y$, we need to update $x$ and $y$ with the following formula:\n",
    "$$\n",
    "x \\gets x + (-u) \\\\\n",
    "y \\gets y + (-v), \n",
    "$$\n",
    "i.e.,\n",
    "$$\n",
    "x \\gets x - \\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial x} \\\\\n",
    "y \\gets y - \\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial y},\n",
    "$$\n",
    "The learning rate $\\eta$ is applied to the increment, i.e., multipled with $\\Delta x$ and $\\Delta y$. Therefore the final optimization formula is:\n",
    "$$\n",
    "x \\gets x - \\eta \\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial x} \\\\\n",
    "y \\gets y - \\eta \\frac{\\partial f^{(1)}(x_0, y_0)}{\\partial y},\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
