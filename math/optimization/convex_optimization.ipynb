{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mathematical Optimization\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\text{minimize} & & f_0(x) \\\\\n",
    "\\text{subject to} & & f_i(x) \\leq b_i, & i = 1, 2, \\dots, m\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "### Notations\n",
    "+ $f_0: \\mathbf{R}^n \\rightarrow \\mathbf{R}$: objective function\n",
    "+ $f_i: \\mathbf{R}^n \\rightarrow \\mathbf{R}$: constraints\n",
    "+ $x = (x_1, \\dots, x_n)$: optimization variable\n",
    "\n",
    "### Convex Optimization\n",
    "A convex optimization is the optimization problems in which objective function and constraints are convex, which means they satisify $f_i(\\alpha x + \\beta y) \\leq \\alpha f_i(x) + \\beta f_i(y)$ for all $x, y \\in \\mathbf{R}^n$ and all $\\alpha, \\beta \\in \\mathbf{R}$ with $\\alpha + \\beta = 1$, $\\alpha \\geq 0$, $\\beta \\geq 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\text{minimize} & & f_0(x) = \\lVert Ax - b \\rVert_2^2 = \\lVert Xw \\rVert_2^2\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "where $$X = \\begin{bmatrix}\n",
    "    1       & X_{11} & \\ldots & X_{n1} \\\\\n",
    "    \\vdots  & \\vdots &        & \\vdots \\\\\n",
    "    1       & X_{1k} & \\ldots & X_{nk} \n",
    "    \\end{bmatrix}$$\n",
    "    \n",
    "+ $n$: number of explantory variables; $k$: number of observations. ($k \\geq n$)\n",
    "\n",
    "+ If $X$ has full column rank, $w$  has an explicit solution in matrix form as: $w^* = (X^T X)^{-1} X^T y$, where $y = X w$.\n",
    "\n",
    "+ Computational time proportional to $n^2 k$; less if structured\n",
    "\n",
    "+ lower bias, higher variance\n",
    "\n",
    "+ Details: [最小二乘线性回归从入门到放弃 :-)](https://www.bilibili.com/video/av13759873)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "[[ 1.  1.  2.]\n",
      " [ 1.  3.  4.]\n",
      " [ 1.  0.  1.]\n",
      " [ 1.  1.  0.]\n",
      " [ 1. -1.  1.]]\n",
      "w_opt\n",
      "[ 2.  3. -1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x1, x2):\n",
    "    return 2 +  3 * x1 - x2\n",
    "\n",
    "x1 = [1, 3, 0, 1, -1]\n",
    "x2 = [2, 4, 1, 0, 1]\n",
    "y = [f(x11, x22) for x11, x22 in zip(x1, x2)]\n",
    "\n",
    "X = np.zeros((5, 3))\n",
    "X[:, 0] = 1\n",
    "X[:, 1] = x1\n",
    "X[:, 2] = x2\n",
    "print('X', X, sep='\\n')\n",
    "\n",
    "w_opt = np.matmul(np.matmul(np.linalg.inv(np.matmul(X.T, X)), X.T ), y)\n",
    "print('w_opt', w_opt, sep='\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Optimization\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\text{minimize} & & c^T x \\\\\n",
    "\\text{subject to} & & a_i^T x \\leq b_i, & i = 1, 2, \\dots, m\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "vectors $c, a_1, \\dots, a_m \\in \\mathbf{R}^n$ and scalars $b_1, \\dots, b_m \\in \\mathbf{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n",
    "Fix the issue when $X^T X$ is singular or nearly singular: Add a small constant **positive** value $\\lambda$ to the diagonl entries of the matrix $X^T X$ before taking its inverse. [Regularization Part 1: Ridge Regression](https://www.youtube.com/watch?v=Q81RR3yKn30)\n",
    "\n",
    "\n",
    "+ high bias, low variance\n",
    "+ make $y$ less sensitive to `x` by decreasing the slope\n",
    "+ smaller sample size (Note that OLS can only be applied to dataset whose size is not less than its variables, while Ridge Regression has no such constraint) => improve the generalization ability of the model\n",
    "\n",
    "The ridge estimator is $$\\beta^{ridge} = (X^T X + \\lambda I_p)^{-1} X^T y$$\n",
    "\n",
    "Note that OLS is $$\\beta^{ols} = (X^T X)^{-1} X^T y$$\n",
    "\n",
    "The ridge estimator $\\beta^{ridge}$ can be seen as a solution to $$\\underset{\\beta \\in R^p}{\\text{minimize}} \\lVert X \\beta  - y \\rVert^2  + \\lambda \\lVert \\beta \\rVert^2$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression\n",
    "Similar to Ridge Regression, except that its penality is $\\lambda \\lvert \\beta \\rvert$. [Regularization Part 2: Lasso Regression\n",
    "](https://www.youtube.com/watch?v=NGf0voTMlcs)\n",
    "\n",
    "+ Ridge Regression does not exclude useless variables, while Lasso Regression does by setting their coefficients to zero\n",
    "\n",
    "\n",
    "The lasso estimator $\\beta^{ridge}$ can be seen as a solution to $$\\underset{\\beta \\in R^p}{\\text{minimize}} \\lVert X \\beta  - y \\rVert^2  + \\lambda \\lvert \\beta \\rvert$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic Net Regression\n",
    "Ridge + Lasso. [Regularization Part 3: Elastic Net Regression](https://www.youtube.com/watch?v=1dKRdX9bfIo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
