{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Proximal Gradient Method\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\text{minimize} & & f(x) = g(x) + h(x)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "+ $g$ convex, differentiable, with $\\text{dom}(g) = \\mathbb{R}^n$. $g(x)^\\prime$ is Lipschitz continuous, which means that there exists a real constant $k \\geq 0$ such that, for all $x_1$ and $x_2$, $\\lvert f(x_2)^\\prime - f(x_1)^\\prime \\rvert \\leq M \\lvert x_2 - x_1  \\rvert$\n",
    "+ $h$ closed, convex, possibly nondifferentiable; $\\textbf{prox}_h$ is inexpensive\n",
    "\n",
    "\n",
    "\n",
    "## Proximal Mapping\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "    \\textbf{prox}_h(x) = \\underset{u}{\\text{argmin}}(\\frac{1}{2} \\lVert u - x \\rVert_2^2 + h(u))\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "### Examples\n",
    "\n",
    "$h(x) = t \\lVert x \\rVert_1$: $\\textbf{prox}_h$ is shrinkage (soft threshold) operation:\n",
    "\n",
    "$$\n",
    "\\textbf{prox}_h(x)_i = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            x_i - t & \\quad x_i \\geq t \\\\\n",
    "            0 & \\quad |x_i| \\leq t \\\\\n",
    "            x_i + t & \\quad x_i \\leq -t \n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "Consider the problem:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{min}} & & g(x) + \\lambda h(x)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "Derivation:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial g(x + t(y - x))}{\\partial t} & = g^\\prime[x + t(y - x))] (y - x) \\\\\n",
    "\\int_0^1 \\frac{\\partial g(x + t(y - x))}{\\partial t}\\,\\mathrm{d}t & = \\int_0^1   g^\\prime[x + t(y - x)] (y - x) \\,\\mathrm{d}t \\\\\n",
    "g(y) - g(x) & = \\int_0^1   g^\\prime[x + t(y - x)] (y - x) \\,\\mathrm{d}t \\\\\n",
    "g(y) & = g(x) + \\int_0^1   g^\\prime[x + t(y - x)] (y - x) \\,\\mathrm{d}t \\\\ \n",
    "& = g(x) + \\int_0^1   \\{g^\\prime[x + t(y - x)] -g^\\prime(x)\\} (y - x) + g^\\prime(x) (y - x) \\,\\mathrm{d}t \\\\\n",
    "& = g(x) + g^\\prime(x) (y - x) + \\int_0^1   \\{g^\\prime[x + t(y - x)] -g^\\prime(x)\\} ( y - x ) \\,\\mathrm{d}t \\\\\n",
    "& \\leq g(x) + g^\\prime(x) (y - x) + \\int_0^1   \\lvert g^\\prime[x + t(y - x)] -g^\\prime(x)\\rvert_1 \\lvert y - x \\rvert_1 \\,\\mathrm{d}t \\\\ \n",
    "& \\leq g(x) + g^\\prime(x) (y - x) + \\int_0^1   Mt \\vert (y - x) \\rvert^2 \\,\\mathrm{d}t \\\\ \n",
    "& \\leq g(x) + g^\\prime(x) (y - x) +  \\frac{M}{2} \\lVert y - x \\rVert_2^2  \\\\ \n",
    "& = \\frac{M}{2} \\left[ y - (x - \\frac{1}{M} g^\\prime(x))   \\right]^2 + \\text{CONST} \\\\\n",
    "& = \\frac{M}{2} ( y - z)^2 + \\text{CONST} \\\\\n",
    "g(x) & = \\frac{M}{2} ( x - z)^2 + \\text{CONST}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "$\\text{CONST}$ has nothing to do with $y$, and thus the solution is $x - \\frac{1}{M}f^\\prime(x)$. We set $z = x - \\frac{1}{M} f^\\prime(x)$. Then, the problem becomes:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{min}} & & \\frac{M}{2} \\lVert x - z \\rVert_2^2 + \\lambda h(x)\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "### Examples\n",
    "Suppose $h(x) = \\lVert x \\rVert_1$ (L1-regularization), the problem is:\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{x}{\\text{min}} & & \\frac{M}{2} \\lVert x - z \\rVert_2^2 + \\lambda \\lVert x \\rVert_1\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "If $x > 0$, \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Function\n",
    "[wiki](https://en.wikipedia.org/wiki/Convex_function#Strongly_convex_functions)\n",
    "### Definition\n",
    "#### 1. Convex function\n",
    "A function is called convex if:\n",
    "$$\n",
    "\\forall x_1, x_2 \\in \\mathbf{X}, \\forall t \\in [0, 1]:  f(t x_1 + (1-t) x_2) \\leq tf(x_1) + (1-t)f(x_2)\n",
    "$$\n",
    "#### 2. Strictly convex function\n",
    "A function is called strictly convex if:\n",
    "$$\n",
    "\\forall x_1 \\ne x_2 \\in \\mathbf{X}, \\forall t \\in [0, 1]:  f(t x_1 + (1-t) x_2) \\lt tf(x_1) + (1-t)f(x_2)\n",
    "$$\n",
    "####  3. Strongly convex functions\n",
    "\n",
    "A strongly convex function is also strictly convex, but not vice versa.\n",
    "\n",
    "There are many definitions:\n",
    "1. A differentiable function $f$ is called strongly convex with parameter $m > 0$ if the following inequality holds for all the points $x$, $y$ in its domain:\n",
    "$$\n",
    "(\\bigtriangledown f(x) - \\bigtriangledown f(y))^T (x - y) \\geq m \\lVert x - y \\rVert_2^2\n",
    "$$\n",
    "2. A equivalent condition is the following:\n",
    "$$\n",
    "f(y) \\geq f(x) + \\bigtriangledown f(x)^T (y - x) + \\frac{m}{2} \\lVert y -x \\rVert_2^2\n",
    "$$\n",
    "3. It is not necessary for a function to be differentiable in order to be strongly convex. A third definition for a strongly convex function, with parameter $m$, is that, for all $x$, $y$ in the domain and $t \\in [0, 1]$,\n",
    "$$\n",
    "f(tx + (1-t)y) \\leq tf(x) + (1-t)f(y) - \\frac{m}{2} t(1-t)\\lVert x - y \\rVert_2^2\n",
    "$$\n",
    "\n",
    "The parameter $m$ is called the *convexity parameter*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proximal Stochastic Gradient Method\n",
    "### Introduction\n",
    "\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\underset{x \\in \\mathbb{R}^d}{\\text{minimize}} & & \\{P(x) \\overset{\\text{def}}{=} F(x) + R(x)\\}\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "\n",
    "$F(x)$ is the average of many smooth component functions $f_i(x)$, i.e.,\n",
    "$$F(x) = \\frac{1}{n}\\sum_{i=1}^n f_i(x)$$,\n",
    "and $R(x)$ is simple but can be nondifferentiable.\n",
    "\n",
    "Assumptions:\n",
    "\n",
    "$R(x)$: lower semicontinuous and convex, $\\text{dom}(R) := \\{ x \\in \\mathbb{R}^d \\vert R(x) < +\\infty \\}$\n",
    "\n",
    "$f_i(x)$: differentiable, and their gradients are Lipschitz continuous:\n",
    "$$\\lvert f_i(x_2)^\\prime - f_i(x_1)^\\prime \\rvert \\leq M_i \\lvert x_2 - x_1  \\rvert$$\n",
    "\n",
    "$F(x)$: differentiable, and their gradients are Lipschitz continuous:\n",
    "$$\\lvert F(x_2)^\\prime - F(x_1)^\\prime \\rvert \\leq M \\lvert x_2 - x_1  \\rvert$$.\n",
    "\n",
    "Moreover, we have $M \\leq \\frac{1}{n} \\sum_{i=1}^n M_i$\n",
    "\n",
    "\n",
    "The overall cost function $P(x)$ is strongly convex, i.e., there exist $μ > 0$ such that for all $x \\in \\text{dom}(R)$ and $y \\in \\mathbb{R}^d$,\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "P (y) ≥ P (x) + \\xi^T (y − x) + \\frac{\\mu}{2} \\lVert y − x \\rVert^2 & & ∀ \\xi \\in ∂P (x)\n",
    "\\end{aligned}\n",
    "\\end{equation*},\n",
    "\n",
    "The strong convexity of $P(x)$ may come from either $F(x)$ or $R(x)$ or both.\n",
    "\n",
    "\n",
    "\n",
    "### 1. standard proximal stochastic gradient method\n",
    "#### 1-1. proximal *full* gradient (Prox-FG)\n",
    "\n",
    "$$x^{(k)} = \\textbf{prox}_{\\eta_k R}\\left(x^{(k-1)} - \\eta_k \\bigtriangledown F(x^{(k-1)})\\right)$$\n",
    "\n",
    "+ $\\eta_k > 0$ is step size, constant(slightly altered each step) or determined by line search\n",
    "\n",
    "\n",
    "+ If the component $n$ (number of $f_i(x)$) is very large, each iteration of the above formula is very expensive since it requires to compute the gradients of all the $n$ component *full* gradient and also their average. $F(x)$ is the average of many smooth component functions. \n",
    "\n",
    "\n",
    "#### 1-2. proximal stochastic gradient (Prox-SG)\n",
    "We draw $i_k$ randomly from $\\{1, \\dots, n\\}$ and take the update:\n",
    "$$x^{(k)} = \\textbf{prox}_{\\eta_k R}\\left(x^{(k-1)} - \\eta_k \\bigtriangledown f_{i_k}(x^{(k-1)})\\right)$$\n",
    "\n",
    "Clearly, we have $\\mathbb{E} \\bigtriangledown f_{i_k}(x^{(k-1)}) = \\bigtriangledown F(x^{(k-1)})$.\n",
    "\n",
    "\n",
    "+ The computational cost per iteration is only $\\frac{1}{n}$ that of the Prox-FG method, since it only evaluates the gradient of a single component function.\n",
    "+ The variance is larger than Prox-FG, and thus the Prox-SG method converges much more slowly than the Prox-FG method.\n",
    "+ The step size $\\eta_k$ has to decay to zero to reduce the variance introduced by random sampling, which leads to slow convergency. Another idea is to compute the full gradient in order to reduce the variance, and it can be achieved by using mini-batches with exponentially growing sizes, but their overall computational cost is still on the same order as full gradients method.\n",
    "\n",
    "\n",
    "#### Analysis\n",
    "\n",
    "give poor sparsity\n",
    "\n",
    "\n",
    "Let $x^* = \\text{arg min}_x P(x)$. Under Assumptions 1 and 2, the Prox-FG method with\n",
    "a constant step size $\\eta_k = \\frac{1}{L}$ generates iterates that satisfy\n",
    "$$P(x_k) − P(x^*) \\leq O \\left( \\left(  \\frac{L - \\mu_F}{L + \\mu_R}  \\right)^k \\right)$$\n",
    "\n",
    "The most interesting case for large-scale\n",
    "applications is when $μ \\ll L$, and the ratio $\\frac{L}{μ}$ is often called the condition number\n",
    "of the problem. In this case, the Prox-FG method needs $O \\left(\\frac{L}{μ} \\text{log}\\frac{1}{\\epsilon}\\right)$\n",
    "iterations to ensure $P(x_k) − P(x^*) \\leq \\epsilon$. Thus the overall complexity of Prox-FG,\n",
    "in terms of the total number of component gradients evaluated to find an $\\epsilon$-accurate\n",
    "solution, is $O \\left(n \\frac{L}{μ} \\text{log}\\frac{1}{\\epsilon}\\right)$. The accelerated Prox-FG methods reduce\n",
    "the complexity to $O\\left(n \\sqrt{\\frac{L}{μ}} \\text{log} \\frac{1}{\\epsilon}\\right)$\n",
    "\n",
    "On the other hand, with a diminishing step size $\\eta_k = \\frac{1}{\\mu k}$, the Prox-SG method\n",
    "converges at a sublinear rate:\n",
    "$$\\mathbb{E}P(x_k) − P(x^*) \\leq O(\\frac{1}{\\mu k})$$\n",
    "Consequently, the total number of component gradient evaluations required by the\n",
    "Prox-SG method to find an $\\epsilon$-accurate solution (in expectation) is $O(\\frac{1}{\\mu \\epsilon})$. This\n",
    "complexity scales poorly in $\\epsilon$ compared with $\\text{log}(\\frac{1}{\\epsilon})$, but it is independent of $n$. Therefore, when $n$ is very large, the Prox-SG method can be more efficient, especially for obtaining low-precision solutions.\n",
    "\n",
    "\n",
    "### 2. Regularized dual averaging (RDA) method\n",
    "[Dual Averaging Methods for Regularized Stochastic Learning and Online Optimization]()\n",
    "\n",
    "#### Soft $l_1$-regularization\n",
    "##### Algorithm\n",
    "\n",
    "**intialize**: set $\\bar{g}_0 = 0$.\n",
    "\n",
    "**for** t = 1, 2, 3, ... **do**\n",
    "1. Given the cost function $f_t(w)$, compute a subgradient $g_t = f^\\prime_t(w)$\n",
    "\n",
    "2. Update the average subgradient(online version of computing the average subgradient).\n",
    "$$\n",
    "\\bar{g}_t = \\frac{t-1}{t} \\bar{g}_{t-1} + \\frac{1}{t}g_t\n",
    "$$\n",
    "\n",
    "3. Compute the next weight vector\n",
    "$$\n",
    "w_{t+1}^{(i)} = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0 & \\quad if |\\bar{g}_t^{(i)}| \\leq \\lambda, \\\\\n",
    "            - \\frac{\\sqrt{t}}{\\gamma} \\left(  \\bar{g}_t^{(i)} - \\lambda \\text{sgn}{(\\bar{g}_t^{(i)})}  \\right) & \\quad \\text{otherwise}\n",
    "        \\end{array}\n",
    "    \\right. \n",
    "    i = 1, \\dots, n.\n",
    "$$,\n",
    "Here, $\\text{sgn}(\\omega)$ equals $1$ if $\\omega \\gt 0$, $-1$ if $\\omega \\leq 0$\n",
    "\n",
    "\n",
    "We do not apply regularization on the bias term $b$.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Proximal Stochastic Variance-reduced Gradient (Prox-SVRG)\n",
    "[A Proximal Stochastic Gradient Method with Progressive Variance Reduction](https://arxiv.org/pdf/1403.4699.pdf)\n",
    "\n",
    "\n",
    "extend the variance reduction technique of SVRG + obtain the complexity $O \\left( \\left( n +  \\frac{L_{max}}{\\mu}  \\right) \\text{log} \\frac{1}{\\epsilon} \\right)$\n",
    "\n",
    "incorporates a weighted sampling strategy. When the sampling probabilities for $i \\in \\{1, \\dots , n\\}$ are proportional to the Lipschitz constants $L_i$ of $∇f_i$ , the Prox-SVRG method has complexity:$$O \\left( \\left( n +  \\frac{L_{avg}}{\\mu}  \\right) \\text{log} \\frac{1}{\\epsilon} \\right)$$, where $L_{avg} = \\frac{1}{n}\\sum_{i=1}^n L_i$\n",
    "\n",
    "reduce the variance of the stochastic gradient by using the variance reudction technique of SVRG instead of increasing batch size gradually\n",
    "\n",
    "converge to the optimum at a geometric rate\n",
    "\n",
    "\n",
    "Several recent works considered various special cases of (1.1) and (1.2) and developed algorithms that enjoy thecomplexity (total number of component gradient evaluations):\n",
    "\n",
    "$$O \\left( \\left( n +  \\frac{L_{max}}{\\mu}  \\right) \\text{log} \\frac{1}{\\epsilon} \\right)$$\n",
    "\n",
    "where $L_{max} = \\text{max}\\{L_1, \\dots,  L_n \\}$. If $L_{max}$ is not significantly larger than L, this complexity is far superior than that of both the Prox-FG and Prox-SG methods. The example is a proximal stochastic dual coordinate ascent(Prox-SDCA) method.\n",
    "\n",
    "\n",
    "#### Motivations\n",
    "+ Prox-FG ignores the fact that $F(x)$ is the average of n component functions\n",
    "+ Prox-SG does not exploit the fact that objective function is actually a deterministic function\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "1. Maintain an estimate $\\tilde{x}$ of the optimal point $x_*$\n",
    "    + Updated after every $m$ Prox-SG iterations($\\tilde{x}$ can be set to be the last iterate $x_m$ or the average of $x_k$ for $m$ iterations, i.e., $\\tilde{x} = \\frac{1}{m} \\sum_{k=1}^m x_k$). \n",
    "\n",
    "2. Compute the full gradient, whenever $\\tilde{x}$ is updated:\n",
    "$$\n",
    "\\tilde{v} = \\bigtriangledown{F(\\tilde{x})} = \\frac{1}{n} \\sum_{i=1}^n {\\bigtriangledown{f_i(\\tilde{x})}}\n",
    "$$\n",
    "\n",
    "3. Compute the next $m$ stochastic gradient directions $v_k$. \n",
    "    + Suppose the next $m$ iterations are initialized with $x_0=\\tilde{x}$\n",
    "    + The next iterations are indexed by $k = 1, \\dots, m$(id of iteration, and each $x_k$ is updated from $x_{k-1}$ inside the $m$ iteration)\n",
    "    + For each $k \\geq 1$, we first randomly pick $i_k \\in \\{ 1, \\dots, n \\}$(n is the number of component functions)\n",
    "    + Compute the direction $v_k$ (which is a stochastic gradient of $F$ at $x_{k-1}$): $v_k = \\bigtriangledown{ f_{ik}(x^{(k-1)}) } - \\bigtriangledown{ f_{ik}(\\tilde{x}) } + \\bigtriangledown{ F(\\tilde{x}) }$\n",
    "    + Note: If we sample $i_k$ from a generatl distribution $\\{ q_1, \\dots, q_n \\}$ instead of uniform sampling, we have $v_k = (\\bigtriangledown{ f_{ik}(x^{(k-1)}) } - \\bigtriangledown{ f_{ik}(\\tilde{x}) }) / (q_{ik} n) + \\bigtriangledown{ F(\\tilde{x}) }$\n",
    "\n",
    "4. Replace $\\bigtriangledown{ f_{ik}(x_{k-1}) }$ in the Prox-SG method with $v_k$, i.e.,\n",
    "$$\n",
    "x^{(k)} = \\textbf{prox}_{\\eta_k R}(x^{(k-1)} - \\eta_k v_k)\n",
    "$$\n",
    "\n",
    "\n",
    "(\n",
    "*Proof: $v_k$ is a stochastic gradient of $F$ at $x_{k-1}$:*\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}v_k & = \\mathbb{E}\\bigtriangledown{ f_{ik}(x^{(k-1)}) } - \\mathbb{E}\\bigtriangledown{ f_{ik}(\\tilde{x}) } + \\bigtriangledown{ F(\\tilde{x}) } \\\\\n",
    "& = \\bigtriangledown{ F(x^{(k-1)}) } - \\bigtriangledown{ F(\\tilde{x}) } + \\bigtriangledown{ F(\\tilde{x}) } \\\\\n",
    "& = \\bigtriangledown{ F(x^{(k-1)}) }\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n",
    "Besides, the variance $\\mathbb{E} \\lVert v_k - \\bigtriangledown{F(x_{k-1})} \\rVert^2$ can be much smaller than $\\mathbb{E} \\lVert \\bigtriangledown{ f_{ik}(x^{k-1}) } - \\bigtriangledown{F(x_{k-1})} \\rVert^2$. In fact, the following inequality holds:\n",
    "$$\n",
    "\\mathbb{E} \\lVert v_k - \\bigtriangledown{F(x_{k-1})} \\rVert^2 \\leq 4 L_{max} [ P(x_{k-1}) - P(x_*) + P(\\tilde{x})  - P(x_*)  ]\n",
    "$$\n",
    "Therefore, **when both $x_{k-1}$ and $\\tilde{x}$ converge to $x_*$, the variance of $v_k$ also converges to zero**. As a result, we can use a **constant step size** and obtain much **faster convergency**.\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "#### Numerical Experiments\n",
    "\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "\\underset{x \\in \\mathbb{R}^d}{\\text{minimize}} & & \\frac{1}{n}\\sum_{i=1}^n \\text{log}(1 + \\text{exp}(-b_i a_i^T x)) + \\frac{\\lambda_2}{2} \\lVert x \\rVert_2^2 + \\lambda_1 \\lVert x \\rVert_1\n",
    "\\end{aligned}\n",
    "\\end{equation*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Orthant Wise Proximal Gradient Method (OWPROX)\n",
    "\n",
    "\n",
    "#### Algorithm\n",
    "##### 1. Outline of OWPROX\n",
    "1. **Input**\" $x_0$(weights)\n",
    "2. for $k = 0, 1, 2, \\dots, K - 1$(total epochs) do \n",
    "3. &nbsp;&nbsp;&nbsp;&nbsp;**if** termination condition is satisified (for example, number of epochs excesses max_epoch) **then**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Return** the (approximate) solution $x_k$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;**if** $x_k$ is closed enough to $x^*$(The regularized objective function value only change a little bit after one epoch). **then**\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x_{k+1} \\leftarrow \\text{reduced space active set prediction step}$ \n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;**else**\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$x_{k+1} \\leftarrow \\text{objective decreasing step}$ \n",
    "\n",
    "##### 2. Objective decreasing step (dual-primal-averaging)\n",
    "1. **Input**: $x_k$(value of weights after k updates), $\\alpha_k$ (learning rate)\n",
    "2. Set $y_0 \\leftarrow x_k$\n",
    "3. **for** $t = 0, 1, 2, \\dots, T - 1$(total number of updating. In the code, it is the number of batches) **do**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;Compute $g_t$(mean gradient of $F$ in mini-batch) as \n",
    "$$\n",
    "g_t \\leftarrow \\frac{1}{\\lVert \\mathscr{B}_t \\rVert} \\sum_{i \\in \\mathscr{B}_t}{\\bigtriangledown{f_i(y_t)}}\n",
    "$$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;Compute average $\\hat{g}_t$ as (Note that when $t$ is $0$, $\\hat{g}_t = g_t$)\n",
    "$$\n",
    "\\hat{g}_t \\leftarrow \\frac{t}{t+1}\\hat{g}_{t-1} + \\frac{1}{t+1}g_t\n",
    "$$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;Compute average $\\hat{y}_t$ as \n",
    "$$\n",
    "\\hat{y}_t \\leftarrow \\frac{t}{t+1}\\hat{y}_{t-1} + \\frac{1}{t+1}y_t\n",
    "$$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;Compute $y_{t+1} = \\textbf{prox}_{\\alpha_k \\lambda \\omega}(\\hat{y}_t - \\alpha_k \\hat{g}_t)$\n",
    "8. **Return** $x_{k+1} \\leftarrow y_T$ \n",
    "\n",
    "##### 2. Objective decreasing step (dual-averaging)\n",
    "1. **Input**: $x_k$(value of weights after k updates), $\\alpha_k$ (learning rate)\n",
    "2. Set $y_0 \\leftarrow x_k$\n",
    "3. **for** $t = 0, 1, 2, \\dots, T - 1$(total number of updating. In the code, it is the number of batches) **do**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;Compute $g_t$(mean gradient of $F$ in mini-batch) as \n",
    "$$\n",
    "g_t \\leftarrow \\frac{1}{\\lVert \\mathscr{B}_t \\rVert} \\sum_{i \\in \\mathscr{B}_t}{\\bigtriangledown{f_i(y_t)}}\n",
    "$$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;Compute average $\\hat{g}_t$ as (Note that when $t$ is $0$, $\\hat{g}_t = g_t$)\n",
    "$$\n",
    "\\hat{g}_t \\leftarrow \\frac{t}{t+1}\\hat{g}_{t-1} + \\frac{1}{t+1}g_t\n",
    "$$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;Compute $y_{t+1} = \\textbf{prox}_{\\alpha_k \\lambda \\omega}(y_t - \\alpha_k \\hat{g}_t)$\n",
    "7. **Return** $x_{k+1} \\leftarrow y_T$ \n",
    "\n",
    "\n",
    "##### 2. Objective decreasing step (batching)\n",
    "1. **Input**: $x_k$(value of weights after k updates), $\\alpha_k$ (learning rate)\n",
    "2. Set $y_0 \\leftarrow x_k$\n",
    "3. **for** $t = 0, 1, 2, \\dots, T - 1$(total number of updating. In the code, it is the number of batches) **do**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;Compute $g_t$(mean gradient of $F$ in mini-batch) as \n",
    "$$\n",
    "g_t \\leftarrow \\frac{1}{\\lVert \\mathscr{B}_t \\rVert} \\sum_{i \\in \\mathscr{B}_t}{\\bigtriangledown{f_i(y_t)}}\n",
    "$$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;Compute $y_{t+1} = \\textbf{prox}_{\\alpha_k \\lambda \\omega}(y_t - \\alpha_k g_t)$\n",
    "6. **Return** $x_{k+1} \\leftarrow y_T$ \n",
    "\n",
    "##### 3. reduced space active set prediction step(dual-averaging, dual-primal-averaging)\n",
    "1. Compute $\\zeta$ as the following:\n",
    "$$\n",
    "\\zeta_i = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad x \\gt 0 \\\\\n",
    "            -1 & \\quad x \\lt 0 \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "2. Set $y_0 \\leftarrow x_k$\n",
    "3. **for** mini-batch $t = 0, 1, 2, \\dots$ **do**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;Compute $g_t$(mean gradient of $F$ in mini-batch) as \n",
    "$$\n",
    "g_t \\leftarrow \\frac{1}{\\lVert \\mathscr{B}_t \\rVert} \\sum_{i \\in \\mathscr{B}_t}{\\bigtriangledown{f_i(y_t)}}\n",
    "$$\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;Compute average $\\hat{g}_t$ as (Note that when $t$ is $0$, $\\hat{g}_t = g_t$)\n",
    "$$\n",
    "\\hat{g}_t \\leftarrow \\frac{t}{t+1}\\hat{g}_{t-1} + \\frac{1}{t+1}g_t\n",
    "$$\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;Compute $\\hat{y}_t$ by using gradient descent with $(y_t, \\hat{g}_t, \\zeta, \\lambda, 0.1 \\alpha)$: (Note that we decrease positive elements more, while increase negative elements more)\n",
    "$$\n",
    "\\hat{y}_t = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            y_t - 0.1 \\alpha (\\hat{g}_{t_i} + \\lambda) & \\quad \\zeta_i \\gt 0 \\\\\n",
    "            y_t - 0.1 \\alpha  (\\hat{g}_{t_i} - \\lambda) & \\quad \\zeta_i \\lt 0 \\\\\n",
    "            y_t  & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right. \\\\\n",
    "$$\n",
    "\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;Compute $\\text{proj}_{y_t}$ from project function with $(\\hat{y}_t, \\zeta)$\n",
    "8. &nbsp;&nbsp;&nbsp;&nbsp;Update non-zeros(determined by $x_k$) in $y_t$ as $\\text{proj}_{y_t}$\n",
    "9. **Return** $x_{k+1} \\leftarrow y_T$ \n",
    "\n",
    "\n",
    "##### 3. reduced space active set prediction step(batching)\n",
    "1. Compute $\\zeta$ as the following:\n",
    "$$\n",
    "\\zeta_i = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            1 & \\quad x \\gt 0 \\\\\n",
    "            -1 & \\quad x \\lt 0 \\\\\n",
    "            0 & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$\n",
    "2. Set $y_0 \\leftarrow x_k$\n",
    "3. **for** mini-batch $t = 0, 1, 2, \\dots$ **do**\n",
    "4. &nbsp;&nbsp;&nbsp;&nbsp;Compute $g_t$(mean gradient of $F$ in mini-batch) as \n",
    "$$\n",
    "g_t \\leftarrow \\frac{1}{\\lVert \\mathscr{B}_t \\rVert} \\sum_{i \\in \\mathscr{B}_t}{\\bigtriangledown{f_i(y_t)}}\n",
    "$$\n",
    "\n",
    "5. &nbsp;&nbsp;&nbsp;&nbsp;Compute $\\hat{y}_t$ by using gradient descent with $(y_t, g_t, \\zeta, \\lambda, 0.1 \\alpha)$: (Note that we decrease positive elements more, while increase negative elements more)\n",
    "$$\n",
    "\\hat{y}_t = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            y_t - 0.1 \\alpha (g_{t_i} + \\lambda) & \\quad \\zeta_i \\gt 0 \\\\\n",
    "            y_t - 0.1 \\alpha  (g_{t_i} - \\lambda) & \\quad \\zeta_i \\lt 0 \\\\\n",
    "            y_t  & \\quad otherwise\n",
    "        \\end{array}\n",
    "    \\right. \\\\\n",
    "$$\n",
    "\n",
    "6. &nbsp;&nbsp;&nbsp;&nbsp;Compute $\\text{proj}_{y_t}$ from project function with $(\\hat{y}_t, \\zeta)$\n",
    "7. &nbsp;&nbsp;&nbsp;&nbsp;Update non-zeros(determined by $x_k$) in $y_t$ as $\\text{proj}_{y_t}$\n",
    "8. **Return** $x_{k+1} \\leftarrow y_T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Codes\n",
    "### Notations\n",
    "1. `parms`: dictionary-form parameters, including max_epoch and batch_size\n",
    "2. `pname`: name of the problem being solving\n",
    "3. `probLoad`: relative file name of dataset => X, y\n",
    "4. `X`: (train_size=num_samples=m, num_features=n)\n",
    "5. `y`: (train_size=num_samples=m, num_classes)\n",
    "6. `expterm`: save $e^{-y_i \\cdot (w^T x_i)}$: `(num_samples, 1)·[(num_samples, num_features)*(num_features, 1)]`\n",
    "7. `sigmoid`: not a function! save $\\frac{e^t}{1 + e^t}$\n",
    "8. `x`: weights which we need to use our method to update. (num_features, 1)\n",
    "9. `norm_l1_x`: $\\lvert x \\rvert_1$\n",
    "10. `nnz`: number of non-zeros in `x`\n",
    "11. `indexes`: `[1, 2, ..., num_samples]`\n",
    "12. `func`: $\\frac{1}{\\text{length(indexes)}} \\sum{ \\text{log}  (1 + \\text{obj.expterm}) }  $\n",
    "13. `labmda`: weighting parameter. The $\\lambda$ in $R(x)=\\lambda \\lvert x \\rvert_1$\n",
    "14. `alpha`: decay after each epoch is finished. The $\\eta_k$ in $x^{(k)} = \\textbf{prox}_{\\eta_k R}\\left(x^{(k-1)} - \\eta_k \\bigtriangledown f_{i_k}(x^{(k-1)})\\right)$\n",
    "15. `grad_f`: $\\bigtriangledown f_{i_k}(x^{(k-1)})$\n",
    "16. `f`: value of the objective function for logistic regression(not the final objective function)\n",
    "17. `F`: value of the final objective function which is $F = f + \\lambda \\lvert x \\rvert_1$\n",
    "18. `calculate_d`: compute the distance from $x^{(k-1)}$ to the proximal gradients of $\\left(x^{(k-1)} - \\eta_k \\bigtriangledown F(x^{(k-1)})\\right)$. The return value is $\\textbf{prox}_{\\eta_k R}{\\left(x^{(k-1)} - \\eta_k \\bigtriangledown F(x^{(k-1)})\\right)} - x^{(k-1)}$\n",
    "\n",
    "\n",
    "\n",
    "### Code Blocks\n",
    "#### 1\n",
    "```\n",
    "% \"setExpterm\" is only used for \"setSigmoid\" and \"func\", while \"setSigmoid\" is only used for \"setDiag\" and \"grad\"\n",
    "fun.setExpterm(x, minibatch_idxes);\n",
    "fun.setSigmoid();\n",
    "```\n",
    "$$\n",
    "\\frac{e^{-y^{(batch)} \\cdot (w^T x^{(batch)})}}{1 + e^{-y^{(batch)} \\cdot (w^T x^{(batch)})}}\n",
    "$$\n",
    "\n",
    "#### 2\n",
    "```\n",
    "fun.setExpterm(x, indexes);\n",
    "fun.setSigmoid();\n",
    "f = fun.func(indexes);\n",
    "```\n",
    "\n",
    "$$\\text{value of objective function for LR} = \\frac{1}{\\text{num_samples}} \\sum_{i=1}^{\\text{num_samples}}{  \\text{log} (1 + e^{-y \\cdot (X x)}) }$$,\n",
    "where `indexes` is `[1, 2, ..., num_samples]`, and thus $y$ is the training labels with the shape of (num_samples, 1), $X$ is the training features with the shape of (num_samples, num_features) and $x$ is a vector of (num_features, 1)\n",
    "\n",
    "\n",
    "#### 3. Compute the gradients of $F$ at mini-batch\n",
    "```\n",
    "\n",
    "fun.setExpterm(x, minibatch_idxes);\n",
    "fun.setSigmoid();\n",
    "grad_f = fun.grad(minibatch_idxes); % (num_features, 1)\n",
    "```\n",
    "$$\\bigtriangledown F(x^{(batch)}) = -\\frac{1}{\\text{batch_size}} \\left(\\left(\\frac{e^{-y^{(batch)} \\cdot (X^{(batch)} x)}}{1 + e^{-y^{(batch)} \\cdot (X^{(batch)} x)}} y^{(batch)}\\right)^T X^{(batch)} \\right)^T$$,\n",
    "\n",
    "\n",
    "#### 4. Compute $\\zeta$\n",
    "```matlab\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "% x =\n",
    "%    -0.3378\n",
    "%     0.2943\n",
    "%    -0.1888\n",
    "%     0.0285\n",
    "%    -0.3344\n",
    "%     0.1020\n",
    "%    -0.2370\n",
    "%     0.1541\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "\n",
    "zeta = zeros(size(x))\n",
    "zeta(x > 0) = 1\n",
    "zeta(x < 0) = -1\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "%     -1\n",
    "%      1\n",
    "%     -1\n",
    "%      1\n",
    "%     -1\n",
    "%      1\n",
    "%     -1\n",
    "%      1\n",
    "%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "```\n",
    "#### 5. Project Function\n",
    "\n",
    "```matlab\n",
    "function [ proj_x, viol_size ] = project(trial_x, zeta)\n",
    "\n",
    "proj_x = zeros(size(trial_x));\n",
    "keep_indexes = ( trial_x .* zeta > 0 ); % compute \"trial_x .* zeta\" first. \"keep_indexes\" is of shape (n, 1) in which has the value of 0 if and only if \"trial_x\" is 0, and 1 otherwise\n",
    "proj_x( keep_indexes ) = trial_x( keep_indexes );\n",
    "viol_size = size(trial_x,1) - sum(keep_indexes); % number of zero elements\n",
    "end\n",
    "\n",
    "```\n",
    "#### 6. Gradient Descent\n",
    "```\n",
    "% lambda is the weighting parameter of |x|_1, alpha is the learning rate\n",
    "% Return the updated x\n",
    "function hat_x = gradient_descent(x, grad_f, zeta, lambda, alpha)\n",
    "grad = zeros(size(grad_f));\n",
    "grad(zeta~=0) = grad_f(zeta~=0); % omit zero gradients\n",
    "grad(zeta>0) = grad(zeta>0) + lambda; % increase the gradients of positive elements by lambda, in order to reduce them more\n",
    "grad(zeta<0) = grad(zeta<0) - lambda; % decrease the gradients of negative elements by lambda, in order to enlarge them more\n",
    "hat_x = x - alpha * grad; \n",
    "end\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "1. What is the project function?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{loss}(x, class) = -\\log\\left(\\frac{\\exp(x[class])}{\\sum_j \\exp(x[j])}\\right)\n",
    "               = -x[class] + \\log\\left(\\sum_j \\exp(x[j])\\right)\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
